{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ToDo\n",
    "----\n",
    "\n",
    "1) faire varier les hyperparamètres et enregistrer les performances\n",
    "- longueur du set d'entrainement\n",
    "- vitesse d'apprentissage : valeur init et taux\n",
    "- batch size ?\n",
    "- \n",
    "\n",
    "2) modifier le modèle :\n",
    "- ajouter des couches tant que 1) le justifie\n",
    "- dropout\n",
    "\n",
    "3) factoriser le code :\n",
    "- créer une classe pour le réseau de neurones (scope + init + placeholders I/O)\n",
    "- créer une fonction de compte rendu : print (si verbose), log, etc => dico des var à logger\n",
    "- ébauche de framework pour la modification des hyperparamètres\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (194661, 28, 28) (194661,)\n",
      "Validation set (9967, 28, 28) (9967,)\n",
      "Test set (9967, 28, 28) (9967,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST_cleaned.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (194661, 784) (194661, 10)\n",
      "Validation set (9967, 784) (9967, 10)\n",
      "Test set (9967, 784) (9967, 10)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels, name='accuracy'):\n",
    "    correct_predictions = tf.equal(tf.argmax(predictions,1), tf.argmax(labels,1))\n",
    "    prediction_accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=name)\n",
    "    return prediction_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "input_size = image_size * image_size\n",
    "hidden_size = 90\n",
    "output_size = num_labels\n",
    "\n",
    "num_steps = 6000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Generic placeholders : it can either run with ttrain / test / valid data\n",
    "    input_data = tf.placeholder(tf.float32, shape=(None, input_size), name='input')\n",
    "    output_data = tf.placeholder(tf.float32, shape=(None, output_size), name='output')\n",
    "\n",
    "    # First layer\n",
    "    with tf.variable_scope('first_layer') as scope:\n",
    "        layer_1_weights = tf.Variable(tf.truncated_normal([input_size, hidden_size]), name='layer_1_weights')\n",
    "        layer_1_biases = tf.Variable(tf.zeros([hidden_size]), name='layer_1_biases')\n",
    "        layer_1_logits = tf.matmul(input_data, layer_1_weights) + layer_1_biases\n",
    "        layer_1_relu = tf.nn.relu(layer_1_logits, name='layer_1_relu')\n",
    "    \n",
    "    # Second layer\n",
    "    with tf.variable_scope('second_layer') as scope:\n",
    "        layer_2_weights = tf.Variable(tf.truncated_normal([hidden_size, output_size]), name='layer_2_weights')\n",
    "        layer_2_biases = tf.Variable(tf.zeros([output_size]), name='layer_2_biases')\n",
    "        layer_2_logits = tf.matmul(layer_1_relu, layer_2_weights) + layer_2_biases\n",
    "    \n",
    "    # Output = prediction\n",
    "    with tf.variable_scope('prediction') as scope:\n",
    "        model_output = tf.nn.softmax(layer_2_logits, name='model_prediction')\n",
    "        model_accuracy = accuracy(model_output, output_data, name='model_accuracy')\n",
    "    \n",
    "    # Loss\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        accuracy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(layer_2_logits, output_data), name='loss')\n",
    "        regularization_loss = tf.nn.l2_loss(layer_1_weights) + tf.nn.l2_loss(layer_1_biases) + \\\n",
    "                              tf.nn.l2_loss(layer_2_weights) + tf.nn.l2_loss(layer_2_biases)\n",
    "        loss = accuracy_loss + 0.001 * regularization_loss\n",
    "    \n",
    "    # Optimizer with dynamic learning rate\n",
    "    with tf.variable_scope('optimizer') as scope:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, num_steps * batch_size, 0.96)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "==> step 100\n",
      "Train set accuracy: 0.696066\n",
      "Validation set accuracy: 0.765225\n",
      "Test set accuracy: 0.765225\n",
      "==> step 200\n",
      "Train set accuracy: 0.653762\n",
      "Validation set accuracy: 0.705428\n",
      "Test set accuracy: 0.705428\n",
      "==> step 300\n",
      "Train set accuracy: 0.729484\n",
      "Validation set accuracy: 0.788502\n",
      "Test set accuracy: 0.788502\n",
      "==> step 400\n",
      "Train set accuracy: 0.756500\n",
      "Validation set accuracy: 0.830440\n",
      "Test set accuracy: 0.830440\n",
      "==> step 500\n",
      "Train set accuracy: 0.761226\n",
      "Validation set accuracy: 0.821210\n",
      "Test set accuracy: 0.821210\n",
      "==> step 600\n",
      "Train set accuracy: 0.767719\n",
      "Validation set accuracy: 0.841678\n",
      "Test set accuracy: 0.841678\n",
      "==> step 700\n",
      "Train set accuracy: 0.773483\n",
      "Validation set accuracy: 0.844888\n",
      "Test set accuracy: 0.844888\n",
      "==> step 800\n",
      "Train set accuracy: 0.784132\n",
      "Validation set accuracy: 0.857430\n",
      "Test set accuracy: 0.857430\n",
      "==> step 900\n",
      "Train set accuracy: 0.782067\n",
      "Validation set accuracy: 0.839771\n",
      "Test set accuracy: 0.839771\n",
      "==> step 1000\n",
      "Train set accuracy: 0.787343\n",
      "Validation set accuracy: 0.858232\n",
      "Test set accuracy: 0.858232\n",
      "==> step 1100\n",
      "Train set accuracy: 0.783280\n",
      "Validation set accuracy: 0.838166\n",
      "Test set accuracy: 0.838166\n",
      "==> step 1200\n",
      "Train set accuracy: 0.806001\n",
      "Validation set accuracy: 0.875690\n",
      "Test set accuracy: 0.875690\n",
      "==> step 1300\n",
      "Train set accuracy: 0.812638\n",
      "Validation set accuracy: 0.879502\n",
      "Test set accuracy: 0.879502\n",
      "==> step 1400\n",
      "Train set accuracy: 0.800093\n",
      "Validation set accuracy: 0.851510\n",
      "Test set accuracy: 0.851510\n",
      "==> step 1500\n",
      "Train set accuracy: 0.813250\n",
      "Validation set accuracy: 0.884318\n",
      "Test set accuracy: 0.884318\n",
      "==> step 1600\n",
      "Train set accuracy: 0.819527\n",
      "Validation set accuracy: 0.888733\n",
      "Test set accuracy: 0.888733\n",
      "==> step 1700\n",
      "Train set accuracy: 0.823041\n",
      "Validation set accuracy: 0.890137\n",
      "Test set accuracy: 0.890137\n",
      "==> step 1800\n",
      "Train set accuracy: 0.824202\n",
      "Validation set accuracy: 0.890238\n",
      "Test set accuracy: 0.890238\n",
      "==> step 1900\n",
      "Train set accuracy: 0.825764\n",
      "Validation set accuracy: 0.893850\n",
      "Test set accuracy: 0.893850\n",
      "==> step 2000\n",
      "Train set accuracy: 0.829041\n",
      "Validation set accuracy: 0.878399\n",
      "Test set accuracy: 0.878399\n",
      "==> step 2100\n",
      "Train set accuracy: 0.833947\n",
      "Validation set accuracy: 0.899769\n",
      "Test set accuracy: 0.899769\n",
      "==> step 2200\n",
      "Train set accuracy: 0.836341\n",
      "Validation set accuracy: 0.901375\n",
      "Test set accuracy: 0.901375\n",
      "==> step 2300\n",
      "Train set accuracy: 0.838011\n",
      "Validation set accuracy: 0.902980\n",
      "Test set accuracy: 0.902980\n",
      "==> step 2400\n",
      "Train set accuracy: 0.837810\n",
      "Validation set accuracy: 0.901375\n",
      "Test set accuracy: 0.901375\n",
      "==> step 2500\n",
      "Train set accuracy: 0.839403\n",
      "Validation set accuracy: 0.900472\n",
      "Test set accuracy: 0.900472\n",
      "==> step 2600\n",
      "Train set accuracy: 0.843949\n",
      "Validation set accuracy: 0.906190\n",
      "Test set accuracy: 0.906190\n",
      "==> step 2700\n",
      "Train set accuracy: 0.842886\n",
      "Validation set accuracy: 0.905187\n",
      "Test set accuracy: 0.905187\n",
      "==> step 2800\n",
      "Train set accuracy: 0.848028\n",
      "Validation set accuracy: 0.910003\n",
      "Test set accuracy: 0.910003\n",
      "==> step 2900\n",
      "Train set accuracy: 0.851419\n",
      "Validation set accuracy: 0.911709\n",
      "Test set accuracy: 0.911709\n",
      "==> step 3000\n",
      "Train set accuracy: 0.853001\n",
      "Validation set accuracy: 0.914016\n",
      "Test set accuracy: 0.914016\n",
      "==> step 3100\n",
      "Train set accuracy: 0.848562\n",
      "Validation set accuracy: 0.907294\n",
      "Test set accuracy: 0.907294\n",
      "==> step 3200\n",
      "Train set accuracy: 0.853771\n",
      "Validation set accuracy: 0.913113\n",
      "Test set accuracy: 0.913113\n",
      "==> step 3300\n",
      "Train set accuracy: 0.855169\n",
      "Validation set accuracy: 0.911508\n",
      "Test set accuracy: 0.911508\n",
      "==> step 3400\n",
      "Train set accuracy: 0.859289\n",
      "Validation set accuracy: 0.917227\n",
      "Test set accuracy: 0.917227\n",
      "==> step 3500\n",
      "Train set accuracy: 0.858585\n",
      "Validation set accuracy: 0.915822\n",
      "Test set accuracy: 0.915822\n",
      "==> step 3600\n",
      "Train set accuracy: 0.858652\n",
      "Validation set accuracy: 0.915622\n",
      "Test set accuracy: 0.915622\n",
      "==> step 3700\n",
      "Train set accuracy: 0.863255\n",
      "Validation set accuracy: 0.920136\n",
      "Test set accuracy: 0.920136\n",
      "==> step 3800\n",
      "Train set accuracy: 0.859808\n",
      "Validation set accuracy: 0.917127\n",
      "Test set accuracy: 0.917127\n",
      "==> step 3900\n",
      "Train set accuracy: 0.861873\n",
      "Validation set accuracy: 0.916926\n",
      "Test set accuracy: 0.916926\n",
      "==> step 4000\n",
      "Train set accuracy: 0.864112\n",
      "Validation set accuracy: 0.920738\n",
      "Test set accuracy: 0.920738\n",
      "==> step 4100\n",
      "Train set accuracy: 0.866578\n",
      "Validation set accuracy: 0.923046\n",
      "Test set accuracy: 0.923046\n",
      "==> step 4200\n",
      "Train set accuracy: 0.866419\n",
      "Validation set accuracy: 0.922043\n",
      "Test set accuracy: 0.922043\n",
      "==> step 4300\n",
      "Train set accuracy: 0.866753\n",
      "Validation set accuracy: 0.921240\n",
      "Test set accuracy: 0.921240\n",
      "==> step 4400\n",
      "Train set accuracy: 0.862407\n",
      "Validation set accuracy: 0.919434\n",
      "Test set accuracy: 0.919434\n",
      "==> step 4500\n",
      "Train set accuracy: 0.865633\n",
      "Validation set accuracy: 0.920738\n",
      "Test set accuracy: 0.920738\n",
      "==> step 4600\n",
      "Train set accuracy: 0.869203\n",
      "Validation set accuracy: 0.923247\n",
      "Test set accuracy: 0.923247\n",
      "==> step 4700\n",
      "Train set accuracy: 0.869984\n",
      "Validation set accuracy: 0.924952\n",
      "Test set accuracy: 0.924952\n",
      "==> step 4800\n",
      "Train set accuracy: 0.872563\n",
      "Validation set accuracy: 0.928564\n",
      "Test set accuracy: 0.928564\n",
      "==> step 4900\n",
      "Train set accuracy: 0.869923\n",
      "Validation set accuracy: 0.925253\n",
      "Test set accuracy: 0.925253\n",
      "==> step 5000\n",
      "Train set accuracy: 0.871171\n",
      "Validation set accuracy: 0.925554\n",
      "Test set accuracy: 0.925554\n",
      "==> step 5100\n",
      "Train set accuracy: 0.871870\n",
      "Validation set accuracy: 0.926457\n",
      "Test set accuracy: 0.926457\n",
      "==> step 5200\n",
      "Train set accuracy: 0.869424\n",
      "Validation set accuracy: 0.924150\n",
      "Test set accuracy: 0.924150\n",
      "==> step 5300\n",
      "Train set accuracy: 0.876657\n",
      "Validation set accuracy: 0.928865\n",
      "Test set accuracy: 0.928865\n",
      "==> step 5400\n",
      "Train set accuracy: 0.877464\n",
      "Validation set accuracy: 0.931875\n",
      "Test set accuracy: 0.931875\n",
      "==> step 5500\n",
      "Train set accuracy: 0.872707\n",
      "Validation set accuracy: 0.925153\n",
      "Test set accuracy: 0.925153\n",
      "==> step 5600\n",
      "Train set accuracy: 0.875979\n",
      "Validation set accuracy: 0.929467\n",
      "Test set accuracy: 0.929467\n",
      "==> step 5700\n",
      "Train set accuracy: 0.873452\n",
      "Validation set accuracy: 0.927260\n",
      "Test set accuracy: 0.927260\n",
      "==> step 5800\n",
      "Train set accuracy: 0.875275\n",
      "Validation set accuracy: 0.927360\n",
      "Test set accuracy: 0.927360\n",
      "==> step 5900\n",
      "Train set accuracy: 0.875820\n",
      "Validation set accuracy: 0.929066\n",
      "Test set accuracy: 0.929066\n",
      "==> step 6000\n",
      "Train set accuracy: 0.874253\n",
      "Validation set accuracy: 0.929066\n",
      "Test set accuracy: 0.929066\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # log the quality of the model\n",
    "    train_accuracy_log = tf.scalar_summary(\"train accuracy\", model_accuracy)\n",
    "    valid_accuracy_log = tf.scalar_summary(\"validation accuracy\", model_accuracy)\n",
    "    test_accuracy_log = tf.scalar_summary(\"test accuracy\", model_accuracy)\n",
    "    loss_log = tf.scalar_summary(\"loss\", loss)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/udacity_logs\", session.graph_def)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(1,num_steps+1):\n",
    "        if (step % 100 == 0):\n",
    "            print(\"==> step %d\" % step)\n",
    "            \n",
    "            #evaluate on the train set\n",
    "            feed_dict = {input_data : train_dataset, output_data : train_labels}\n",
    "            acc, acc_str = session.run([model_accuracy, train_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Train set accuracy: %f\" % acc)\n",
    "            \n",
    "            #evaluate on the validation set\n",
    "            feed_dict = {input_data : valid_dataset, output_data : valid_labels}\n",
    "            acc, acc_str = session.run([model_accuracy, valid_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Validation set accuracy: %f\" % acc)\n",
    "            \n",
    "            #evaluate on the test set\n",
    "            feed_dict = {input_data : test_dataset, output_data : test_labels}\n",
    "            acc, acc_str = session.run([model_accuracy, test_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Test set accuracy: %f\" % acc)\n",
    "        else:\n",
    "            # Generate a minibatch.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {input_data : batch_data, output_data : batch_labels}\n",
    "            session.run(optimizer, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_subset(dataset, labels, size):\n",
    "    subset_indices = np.random.choice(labels.shape[0],size)\n",
    "    shuffled_dataset = dataset[subset_indices]\n",
    "    shuffled_labels = labels[subset_indices]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "input_size = image_size * image_size\n",
    "hidden_size = 90\n",
    "output_size = num_labels\n",
    "\n",
    "num_epoch = 20\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Generic placeholders : it can either run with ttrain / test / valid data\n",
    "    input_data = tf.placeholder(tf.float32, shape=(None, input_size), name='input')\n",
    "    output_data = tf.placeholder(tf.float32, shape=(None, output_size), name='output')\n",
    "\n",
    "    # First layer\n",
    "    with tf.variable_scope('first_layer') as scope:\n",
    "        layer_1_weights = tf.Variable(tf.truncated_normal([input_size, hidden_size]), name='layer_1_weights')\n",
    "        layer_1_biases = tf.Variable(tf.zeros([hidden_size]), name='layer_1_biases')\n",
    "        layer_1_logits = tf.matmul(input_data, layer_1_weights) + layer_1_biases\n",
    "        layer_1_relu = tf.nn.relu(layer_1_logits, name='layer_1_relu')\n",
    "    \n",
    "    # Second layer\n",
    "    with tf.variable_scope('second_layer') as scope:\n",
    "        layer_2_weights = tf.Variable(tf.truncated_normal([hidden_size, output_size]), name='layer_2_weights')\n",
    "        layer_2_biases = tf.Variable(tf.zeros([output_size]), name='layer_2_biases')\n",
    "        layer_2_logits = tf.matmul(layer_1_relu, layer_2_weights) + layer_2_biases\n",
    "    \n",
    "    # Output = prediction\n",
    "    with tf.variable_scope('prediction') as scope:\n",
    "        model_output = tf.nn.softmax(layer_2_logits, name='model_prediction')\n",
    "        model_accuracy = accuracy(model_output, output_data, name='model_accuracy')\n",
    "    \n",
    "    # Loss\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        accuracy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(layer_2_logits, output_data), name='loss')\n",
    "        regularization_loss = tf.nn.l2_loss(layer_1_weights) + tf.nn.l2_loss(layer_1_biases) + \\\n",
    "                              tf.nn.l2_loss(layer_2_weights) + tf.nn.l2_loss(layer_2_biases)\n",
    "        loss = accuracy_loss + 0.001 * regularization_loss\n",
    "    \n",
    "    # Optimizer with dynamic learning rate\n",
    "    with tf.variable_scope('optimizer') as scope:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        subset_size = tf.Variable(train_dataset.shape[0], trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, num_epoch * subset_size // batch_size, 0.96)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 2 batches\n",
      "= 256 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 39 steps of training\n",
      "Train set accuracy: 0.769531\n",
      "Validation set accuracy: 0.603391\n",
      "Test set accuracy: 0.603391\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 4 batches\n",
      "= 512 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 79 steps of training\n",
      "Train set accuracy: 0.902344\n",
      "Validation set accuracy: 0.710244\n",
      "Test set accuracy: 0.710244\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 6 batches\n",
      "= 768 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 119 steps of training\n",
      "Train set accuracy: 0.942708\n",
      "Validation set accuracy: 0.733621\n",
      "Test set accuracy: 0.733621\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 8 batches\n",
      "= 1024 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 159 steps of training\n",
      "Train set accuracy: 0.958008\n",
      "Validation set accuracy: 0.775158\n",
      "Test set accuracy: 0.775158\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 10 batches\n",
      "= 1280 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 199 steps of training\n",
      "Train set accuracy: 0.968750\n",
      "Validation set accuracy: 0.769038\n",
      "Test set accuracy: 0.769038\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 12 batches\n",
      "= 1536 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 239 steps of training\n",
      "Train set accuracy: 0.980469\n",
      "Validation set accuracy: 0.779372\n",
      "Test set accuracy: 0.779372\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 14 batches\n",
      "= 1792 samples\n",
      "= 0 % of the full train dataset size\n",
      "= 279 steps of training\n",
      "Train set accuracy: 0.975446\n",
      "Validation set accuracy: 0.792114\n",
      "Test set accuracy: 0.792114\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 16 batches\n",
      "= 2048 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 319 steps of training\n",
      "Train set accuracy: 0.966309\n",
      "Validation set accuracy: 0.786495\n",
      "Test set accuracy: 0.786495\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 18 batches\n",
      "= 2304 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 359 steps of training\n",
      "Train set accuracy: 0.956597\n",
      "Validation set accuracy: 0.788803\n",
      "Test set accuracy: 0.788803\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 20 batches\n",
      "= 2560 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 399 steps of training\n",
      "Train set accuracy: 0.975391\n",
      "Validation set accuracy: 0.802147\n",
      "Test set accuracy: 0.802147\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 22 batches\n",
      "= 2816 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 439 steps of training\n",
      "Train set accuracy: 0.946733\n",
      "Validation set accuracy: 0.776362\n",
      "Test set accuracy: 0.776362\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 24 batches\n",
      "= 3072 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 479 steps of training\n",
      "Train set accuracy: 0.974284\n",
      "Validation set accuracy: 0.812983\n",
      "Test set accuracy: 0.812983\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 26 batches\n",
      "= 3328 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 519 steps of training\n",
      "Train set accuracy: 0.960637\n",
      "Validation set accuracy: 0.814287\n",
      "Test set accuracy: 0.814287\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 28 batches\n",
      "= 3584 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 559 steps of training\n",
      "Train set accuracy: 0.960938\n",
      "Validation set accuracy: 0.813886\n",
      "Test set accuracy: 0.813886\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 30 batches\n",
      "= 3840 samples\n",
      "= 1 % of the full train dataset size\n",
      "= 599 steps of training\n",
      "Train set accuracy: 0.950781\n",
      "Validation set accuracy: 0.813986\n",
      "Test set accuracy: 0.813986\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 32 batches\n",
      "= 4096 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 639 steps of training\n",
      "Train set accuracy: 0.963135\n",
      "Validation set accuracy: 0.821611\n",
      "Test set accuracy: 0.821611\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 34 batches\n",
      "= 4352 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 679 steps of training\n",
      "Train set accuracy: 0.960248\n",
      "Validation set accuracy: 0.801746\n",
      "Test set accuracy: 0.801746\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 36 batches\n",
      "= 4608 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 719 steps of training\n",
      "Train set accuracy: 0.949653\n",
      "Validation set accuracy: 0.826327\n",
      "Test set accuracy: 0.826327\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 38 batches\n",
      "= 4864 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 759 steps of training\n",
      "Train set accuracy: 0.958882\n",
      "Validation set accuracy: 0.824822\n",
      "Test set accuracy: 0.824822\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 40 batches\n",
      "= 5120 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 799 steps of training\n",
      "Train set accuracy: 0.956836\n",
      "Validation set accuracy: 0.828635\n",
      "Test set accuracy: 0.828635\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 42 batches\n",
      "= 5376 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 839 steps of training\n",
      "Train set accuracy: 0.934710\n",
      "Validation set accuracy: 0.808267\n",
      "Test set accuracy: 0.808267\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 44 batches\n",
      "= 5632 samples\n",
      "= 2 % of the full train dataset size\n",
      "= 879 steps of training\n",
      "Train set accuracy: 0.954545\n",
      "Validation set accuracy: 0.840775\n",
      "Test set accuracy: 0.840775\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 46 batches\n",
      "= 5888 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 919 steps of training\n",
      "Train set accuracy: 0.949558\n",
      "Validation set accuracy: 0.837664\n",
      "Test set accuracy: 0.837664\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 48 batches\n",
      "= 6144 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 959 steps of training\n",
      "Train set accuracy: 0.931641\n",
      "Validation set accuracy: 0.808368\n",
      "Test set accuracy: 0.808368\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 50 batches\n",
      "= 6400 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 999 steps of training\n",
      "Train set accuracy: 0.948125\n",
      "Validation set accuracy: 0.834855\n",
      "Test set accuracy: 0.834855\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 52 batches\n",
      "= 6656 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 1039 steps of training\n",
      "Train set accuracy: 0.938101\n",
      "Validation set accuracy: 0.842681\n",
      "Test set accuracy: 0.842681\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 54 batches\n",
      "= 6912 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 1079 steps of training\n",
      "Train set accuracy: 0.935764\n",
      "Validation set accuracy: 0.845390\n",
      "Test set accuracy: 0.845390\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 56 batches\n",
      "= 7168 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 1119 steps of training\n",
      "Train set accuracy: 0.924944\n",
      "Validation set accuracy: 0.841678\n",
      "Test set accuracy: 0.841678\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 58 batches\n",
      "= 7424 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 1159 steps of training\n",
      "Train set accuracy: 0.936288\n",
      "Validation set accuracy: 0.849102\n",
      "Test set accuracy: 0.849102\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 60 batches\n",
      "= 7680 samples\n",
      "= 3 % of the full train dataset size\n",
      "= 1199 steps of training\n",
      "Train set accuracy: 0.930339\n",
      "Validation set accuracy: 0.834654\n",
      "Test set accuracy: 0.834654\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 62 batches\n",
      "= 7936 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1239 steps of training\n",
      "Train set accuracy: 0.938886\n",
      "Validation set accuracy: 0.850105\n",
      "Test set accuracy: 0.850105\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 64 batches\n",
      "= 8192 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1279 steps of training\n",
      "Train set accuracy: 0.930664\n",
      "Validation set accuracy: 0.832748\n",
      "Test set accuracy: 0.832748\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 66 batches\n",
      "= 8448 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1319 steps of training\n",
      "Train set accuracy: 0.929451\n",
      "Validation set accuracy: 0.856828\n",
      "Test set accuracy: 0.856828\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 68 batches\n",
      "= 8704 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1359 steps of training\n",
      "Train set accuracy: 0.913718\n",
      "Validation set accuracy: 0.824621\n",
      "Test set accuracy: 0.824621\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 70 batches\n",
      "= 8960 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1399 steps of training\n",
      "Train set accuracy: 0.925335\n",
      "Validation set accuracy: 0.841377\n",
      "Test set accuracy: 0.841377\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 72 batches\n",
      "= 9216 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1439 steps of training\n",
      "Train set accuracy: 0.933377\n",
      "Validation set accuracy: 0.858232\n",
      "Test set accuracy: 0.858232\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 74 batches\n",
      "= 9472 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1479 steps of training\n",
      "Train set accuracy: 0.919764\n",
      "Validation set accuracy: 0.862145\n",
      "Test set accuracy: 0.862145\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 76 batches\n",
      "= 9728 samples\n",
      "= 4 % of the full train dataset size\n",
      "= 1519 steps of training\n",
      "Train set accuracy: 0.917146\n",
      "Validation set accuracy: 0.861844\n",
      "Test set accuracy: 0.861844\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 78 batches\n",
      "= 9984 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1559 steps of training\n",
      "Train set accuracy: 0.921174\n",
      "Validation set accuracy: 0.858834\n",
      "Test set accuracy: 0.858834\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 80 batches\n",
      "= 10240 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1599 steps of training\n",
      "Train set accuracy: 0.930957\n",
      "Validation set accuracy: 0.865356\n",
      "Test set accuracy: 0.865356\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 82 batches\n",
      "= 10496 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1639 steps of training\n",
      "Train set accuracy: 0.919207\n",
      "Validation set accuracy: 0.868566\n",
      "Test set accuracy: 0.868566\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 84 batches\n",
      "= 10752 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1679 steps of training\n",
      "Train set accuracy: 0.935733\n",
      "Validation set accuracy: 0.872178\n",
      "Test set accuracy: 0.872178\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 86 batches\n",
      "= 11008 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1719 steps of training\n",
      "Train set accuracy: 0.914880\n",
      "Validation set accuracy: 0.871175\n",
      "Test set accuracy: 0.871175\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 88 batches\n",
      "= 11264 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1759 steps of training\n",
      "Train set accuracy: 0.927379\n",
      "Validation set accuracy: 0.868366\n",
      "Test set accuracy: 0.868366\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 90 batches\n",
      "= 11520 samples\n",
      "= 5 % of the full train dataset size\n",
      "= 1799 steps of training\n",
      "Train set accuracy: 0.925260\n",
      "Validation set accuracy: 0.868867\n",
      "Test set accuracy: 0.868867\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 92 batches\n",
      "= 11776 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 1839 steps of training\n",
      "Train set accuracy: 0.919667\n",
      "Validation set accuracy: 0.866961\n",
      "Test set accuracy: 0.866961\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 94 batches\n",
      "= 12032 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 1879 steps of training\n",
      "Train set accuracy: 0.927111\n",
      "Validation set accuracy: 0.875790\n",
      "Test set accuracy: 0.875790\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 96 batches\n",
      "= 12288 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 1919 steps of training\n",
      "Train set accuracy: 0.923665\n",
      "Validation set accuracy: 0.879302\n",
      "Test set accuracy: 0.879302\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 98 batches\n",
      "= 12544 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 1959 steps of training\n",
      "Train set accuracy: 0.918208\n",
      "Validation set accuracy: 0.874285\n",
      "Test set accuracy: 0.874285\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 100 batches\n",
      "= 12800 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 1999 steps of training\n",
      "Train set accuracy: 0.923750\n",
      "Validation set accuracy: 0.874385\n",
      "Test set accuracy: 0.874385\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 102 batches\n",
      "= 13056 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 2039 steps of training\n",
      "Train set accuracy: 0.921798\n",
      "Validation set accuracy: 0.878499\n",
      "Test set accuracy: 0.878499\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 104 batches\n",
      "= 13312 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 2079 steps of training\n",
      "Train set accuracy: 0.920298\n",
      "Validation set accuracy: 0.881208\n",
      "Test set accuracy: 0.881208\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 106 batches\n",
      "= 13568 samples\n",
      "= 6 % of the full train dataset size\n",
      "= 2119 steps of training\n",
      "Train set accuracy: 0.921285\n",
      "Validation set accuracy: 0.859637\n",
      "Test set accuracy: 0.859637\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 108 batches\n",
      "= 13824 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2159 steps of training\n",
      "Train set accuracy: 0.917896\n",
      "Validation set accuracy: 0.886024\n",
      "Test set accuracy: 0.886024\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 110 batches\n",
      "= 14080 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2199 steps of training\n",
      "Train set accuracy: 0.924716\n",
      "Validation set accuracy: 0.884017\n",
      "Test set accuracy: 0.884017\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 112 batches\n",
      "= 14336 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2239 steps of training\n",
      "Train set accuracy: 0.917201\n",
      "Validation set accuracy: 0.885623\n",
      "Test set accuracy: 0.885623\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 114 batches\n",
      "= 14592 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2279 steps of training\n",
      "Train set accuracy: 0.924890\n",
      "Validation set accuracy: 0.867563\n",
      "Test set accuracy: 0.867563\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 116 batches\n",
      "= 14848 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2319 steps of training\n",
      "Train set accuracy: 0.913052\n",
      "Validation set accuracy: 0.883917\n",
      "Test set accuracy: 0.883917\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 118 batches\n",
      "= 15104 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2359 steps of training\n",
      "Train set accuracy: 0.926179\n",
      "Validation set accuracy: 0.888432\n",
      "Test set accuracy: 0.888432\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 120 batches\n",
      "= 15360 samples\n",
      "= 7 % of the full train dataset size\n",
      "= 2399 steps of training\n",
      "Train set accuracy: 0.915820\n",
      "Validation set accuracy: 0.888933\n",
      "Test set accuracy: 0.888933\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 122 batches\n",
      "= 15616 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2439 steps of training\n",
      "Train set accuracy: 0.921747\n",
      "Validation set accuracy: 0.887830\n",
      "Test set accuracy: 0.887830\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 124 batches\n",
      "= 15872 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2479 steps of training\n",
      "Train set accuracy: 0.919985\n",
      "Validation set accuracy: 0.891040\n",
      "Test set accuracy: 0.891040\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 126 batches\n",
      "= 16128 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2519 steps of training\n",
      "Train set accuracy: 0.918589\n",
      "Validation set accuracy: 0.887128\n",
      "Test set accuracy: 0.887128\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 128 batches\n",
      "= 16384 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2559 steps of training\n",
      "Train set accuracy: 0.927002\n",
      "Validation set accuracy: 0.894151\n",
      "Test set accuracy: 0.894151\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 130 batches\n",
      "= 16640 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2599 steps of training\n",
      "Train set accuracy: 0.921214\n",
      "Validation set accuracy: 0.888733\n",
      "Test set accuracy: 0.888733\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 132 batches\n",
      "= 16896 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2639 steps of training\n",
      "Train set accuracy: 0.926255\n",
      "Validation set accuracy: 0.890539\n",
      "Test set accuracy: 0.890539\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 134 batches\n",
      "= 17152 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2679 steps of training\n",
      "Train set accuracy: 0.922400\n",
      "Validation set accuracy: 0.893147\n",
      "Test set accuracy: 0.893147\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 136 batches\n",
      "= 17408 samples\n",
      "= 8 % of the full train dataset size\n",
      "= 2719 steps of training\n",
      "Train set accuracy: 0.920209\n",
      "Validation set accuracy: 0.872279\n",
      "Test set accuracy: 0.872279\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 138 batches\n",
      "= 17664 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2759 steps of training\n",
      "Train set accuracy: 0.918761\n",
      "Validation set accuracy: 0.889435\n",
      "Test set accuracy: 0.889435\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 140 batches\n",
      "= 17920 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2799 steps of training\n",
      "Train set accuracy: 0.923214\n",
      "Validation set accuracy: 0.877797\n",
      "Test set accuracy: 0.877797\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 142 batches\n",
      "= 18176 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2839 steps of training\n",
      "Train set accuracy: 0.923911\n",
      "Validation set accuracy: 0.893749\n",
      "Test set accuracy: 0.893749\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 144 batches\n",
      "= 18432 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2879 steps of training\n",
      "Train set accuracy: 0.924913\n",
      "Validation set accuracy: 0.895756\n",
      "Test set accuracy: 0.895756\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 146 batches\n",
      "= 18688 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2919 steps of training\n",
      "Train set accuracy: 0.920484\n",
      "Validation set accuracy: 0.892746\n",
      "Test set accuracy: 0.892746\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 148 batches\n",
      "= 18944 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2959 steps of training\n",
      "Train set accuracy: 0.921769\n",
      "Validation set accuracy: 0.895455\n",
      "Test set accuracy: 0.895455\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 150 batches\n",
      "= 19200 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 2999 steps of training\n",
      "Train set accuracy: 0.922344\n",
      "Validation set accuracy: 0.894351\n",
      "Test set accuracy: 0.894351\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 152 batches\n",
      "= 19456 samples\n",
      "= 9 % of the full train dataset size\n",
      "= 3039 steps of training\n",
      "Train set accuracy: 0.919922\n",
      "Validation set accuracy: 0.895756\n",
      "Test set accuracy: 0.895756\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 154 batches\n",
      "= 19712 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3079 steps of training\n",
      "Train set accuracy: 0.918831\n",
      "Validation set accuracy: 0.878499\n",
      "Test set accuracy: 0.878499\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 156 batches\n",
      "= 19968 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3119 steps of training\n",
      "Train set accuracy: 0.910357\n",
      "Validation set accuracy: 0.896258\n",
      "Test set accuracy: 0.896258\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 158 batches\n",
      "= 20224 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3159 steps of training\n",
      "Train set accuracy: 0.917326\n",
      "Validation set accuracy: 0.898465\n",
      "Test set accuracy: 0.898465\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 160 batches\n",
      "= 20480 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3199 steps of training\n",
      "Train set accuracy: 0.922412\n",
      "Validation set accuracy: 0.898365\n",
      "Test set accuracy: 0.898365\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 162 batches\n",
      "= 20736 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3239 steps of training\n",
      "Train set accuracy: 0.922454\n",
      "Validation set accuracy: 0.898365\n",
      "Test set accuracy: 0.898365\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 164 batches\n",
      "= 20992 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3279 steps of training\n",
      "Train set accuracy: 0.927306\n",
      "Validation set accuracy: 0.901174\n",
      "Test set accuracy: 0.901174\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 166 batches\n",
      "= 21248 samples\n",
      "= 10 % of the full train dataset size\n",
      "= 3319 steps of training\n",
      "Train set accuracy: 0.918769\n",
      "Validation set accuracy: 0.901274\n",
      "Test set accuracy: 0.901274\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 168 batches\n",
      "= 21504 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3359 steps of training\n",
      "Train set accuracy: 0.922805\n",
      "Validation set accuracy: 0.896258\n",
      "Test set accuracy: 0.896258\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 170 batches\n",
      "= 21760 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3399 steps of training\n",
      "Train set accuracy: 0.917877\n",
      "Validation set accuracy: 0.899569\n",
      "Test set accuracy: 0.899569\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 172 batches\n",
      "= 22016 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3439 steps of training\n",
      "Train set accuracy: 0.917333\n",
      "Validation set accuracy: 0.897562\n",
      "Test set accuracy: 0.897562\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 174 batches\n",
      "= 22272 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3479 steps of training\n",
      "Train set accuracy: 0.924569\n",
      "Validation set accuracy: 0.903080\n",
      "Test set accuracy: 0.903080\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 176 batches\n",
      "= 22528 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3519 steps of training\n",
      "Train set accuracy: 0.923162\n",
      "Validation set accuracy: 0.904384\n",
      "Test set accuracy: 0.904384\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 178 batches\n",
      "= 22784 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3559 steps of training\n",
      "Train set accuracy: 0.924552\n",
      "Validation set accuracy: 0.903682\n",
      "Test set accuracy: 0.903682\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 180 batches\n",
      "= 23040 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3599 steps of training\n",
      "Train set accuracy: 0.923090\n",
      "Validation set accuracy: 0.906491\n",
      "Test set accuracy: 0.906491\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 182 batches\n",
      "= 23296 samples\n",
      "= 11 % of the full train dataset size\n",
      "= 3639 steps of training\n",
      "Train set accuracy: 0.922175\n",
      "Validation set accuracy: 0.904485\n",
      "Test set accuracy: 0.904485\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 184 batches\n",
      "= 23552 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3679 steps of training\n",
      "Train set accuracy: 0.925739\n",
      "Validation set accuracy: 0.904986\n",
      "Test set accuracy: 0.904986\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 186 batches\n",
      "= 23808 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3719 steps of training\n",
      "Train set accuracy: 0.926873\n",
      "Validation set accuracy: 0.903281\n",
      "Test set accuracy: 0.903281\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 188 batches\n",
      "= 24064 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3759 steps of training\n",
      "Train set accuracy: 0.924535\n",
      "Validation set accuracy: 0.905889\n",
      "Test set accuracy: 0.905889\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 190 batches\n",
      "= 24320 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3799 steps of training\n",
      "Train set accuracy: 0.922615\n",
      "Validation set accuracy: 0.906592\n",
      "Test set accuracy: 0.906592\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 192 batches\n",
      "= 24576 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3839 steps of training\n",
      "Train set accuracy: 0.921183\n",
      "Validation set accuracy: 0.890739\n",
      "Test set accuracy: 0.890739\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 194 batches\n",
      "= 24832 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3879 steps of training\n",
      "Train set accuracy: 0.919660\n",
      "Validation set accuracy: 0.905087\n",
      "Test set accuracy: 0.905087\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 196 batches\n",
      "= 25088 samples\n",
      "= 12 % of the full train dataset size\n",
      "= 3919 steps of training\n",
      "Train set accuracy: 0.921915\n",
      "Validation set accuracy: 0.909100\n",
      "Test set accuracy: 0.909100\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 198 batches\n",
      "= 25344 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 3959 steps of training\n",
      "Train set accuracy: 0.918048\n",
      "Validation set accuracy: 0.906592\n",
      "Test set accuracy: 0.906592\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 200 batches\n",
      "= 25600 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 3999 steps of training\n",
      "Train set accuracy: 0.927461\n",
      "Validation set accuracy: 0.907996\n",
      "Test set accuracy: 0.907996\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 202 batches\n",
      "= 25856 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 4039 steps of training\n",
      "Train set accuracy: 0.924930\n",
      "Validation set accuracy: 0.912511\n",
      "Test set accuracy: 0.912511\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 204 batches\n",
      "= 26112 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 4079 steps of training\n",
      "Train set accuracy: 0.912492\n",
      "Validation set accuracy: 0.905990\n",
      "Test set accuracy: 0.905990\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 206 batches\n",
      "= 26368 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 4119 steps of training\n",
      "Train set accuracy: 0.921079\n",
      "Validation set accuracy: 0.889937\n",
      "Test set accuracy: 0.889937\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 208 batches\n",
      "= 26624 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 4159 steps of training\n",
      "Train set accuracy: 0.926570\n",
      "Validation set accuracy: 0.910103\n",
      "Test set accuracy: 0.910103\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 210 batches\n",
      "= 26880 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 4199 steps of training\n",
      "Train set accuracy: 0.914546\n",
      "Validation set accuracy: 0.903782\n",
      "Test set accuracy: 0.903782\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 212 batches\n",
      "= 27136 samples\n",
      "= 13 % of the full train dataset size\n",
      "= 4239 steps of training\n",
      "Train set accuracy: 0.922059\n",
      "Validation set accuracy: 0.908598\n",
      "Test set accuracy: 0.908598\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 214 batches\n",
      "= 27392 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4279 steps of training\n",
      "Train set accuracy: 0.923116\n",
      "Validation set accuracy: 0.911307\n",
      "Test set accuracy: 0.911307\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 216 batches\n",
      "= 27648 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4319 steps of training\n",
      "Train set accuracy: 0.918330\n",
      "Validation set accuracy: 0.907294\n",
      "Test set accuracy: 0.907294\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 218 batches\n",
      "= 27904 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4359 steps of training\n",
      "Train set accuracy: 0.921875\n",
      "Validation set accuracy: 0.910505\n",
      "Test set accuracy: 0.910505\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 220 batches\n",
      "= 28160 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4399 steps of training\n",
      "Train set accuracy: 0.922763\n",
      "Validation set accuracy: 0.910806\n",
      "Test set accuracy: 0.910806\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 222 batches\n",
      "= 28416 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4439 steps of training\n",
      "Train set accuracy: 0.914766\n",
      "Validation set accuracy: 0.907495\n",
      "Test set accuracy: 0.907495\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 224 batches\n",
      "= 28672 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4479 steps of training\n",
      "Train set accuracy: 0.922468\n",
      "Validation set accuracy: 0.894050\n",
      "Test set accuracy: 0.894050\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 226 batches\n",
      "= 28928 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4519 steps of training\n",
      "Train set accuracy: 0.919179\n",
      "Validation set accuracy: 0.912411\n",
      "Test set accuracy: 0.912411\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 228 batches\n",
      "= 29184 samples\n",
      "= 14 % of the full train dataset size\n",
      "= 4559 steps of training\n",
      "Train set accuracy: 0.918723\n",
      "Validation set accuracy: 0.910906\n",
      "Test set accuracy: 0.910906\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 230 batches\n",
      "= 29440 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4599 steps of training\n",
      "Train set accuracy: 0.915149\n",
      "Validation set accuracy: 0.911608\n",
      "Test set accuracy: 0.911608\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 232 batches\n",
      "= 29696 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4639 steps of training\n",
      "Train set accuracy: 0.921538\n",
      "Validation set accuracy: 0.915622\n",
      "Test set accuracy: 0.915622\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 234 batches\n",
      "= 29952 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4679 steps of training\n",
      "Train set accuracy: 0.920406\n",
      "Validation set accuracy: 0.912010\n",
      "Test set accuracy: 0.912010\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 236 batches\n",
      "= 30208 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4719 steps of training\n",
      "Train set accuracy: 0.916611\n",
      "Validation set accuracy: 0.911107\n",
      "Test set accuracy: 0.911107\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 238 batches\n",
      "= 30464 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4759 steps of training\n",
      "Train set accuracy: 0.918133\n",
      "Validation set accuracy: 0.908899\n",
      "Test set accuracy: 0.908899\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 240 batches\n",
      "= 30720 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4799 steps of training\n",
      "Train set accuracy: 0.913737\n",
      "Validation set accuracy: 0.897662\n",
      "Test set accuracy: 0.897662\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 242 batches\n",
      "= 30976 samples\n",
      "= 15 % of the full train dataset size\n",
      "= 4839 steps of training\n",
      "Train set accuracy: 0.914773\n",
      "Validation set accuracy: 0.912311\n",
      "Test set accuracy: 0.912311\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 244 batches\n",
      "= 31232 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 4879 steps of training\n",
      "Train set accuracy: 0.920658\n",
      "Validation set accuracy: 0.915822\n",
      "Test set accuracy: 0.915822\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 246 batches\n",
      "= 31488 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 4919 steps of training\n",
      "Train set accuracy: 0.920986\n",
      "Validation set accuracy: 0.914217\n",
      "Test set accuracy: 0.914217\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 248 batches\n",
      "= 31744 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 4959 steps of training\n",
      "Train set accuracy: 0.918284\n",
      "Validation set accuracy: 0.917428\n",
      "Test set accuracy: 0.917428\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 250 batches\n",
      "= 32000 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 4999 steps of training\n",
      "Train set accuracy: 0.913688\n",
      "Validation set accuracy: 0.910304\n",
      "Test set accuracy: 0.910304\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 252 batches\n",
      "= 32256 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 5039 steps of training\n",
      "Train set accuracy: 0.919674\n",
      "Validation set accuracy: 0.913916\n",
      "Test set accuracy: 0.913916\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 254 batches\n",
      "= 32512 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 5079 steps of training\n",
      "Train set accuracy: 0.923720\n",
      "Validation set accuracy: 0.915622\n",
      "Test set accuracy: 0.915622\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 256 batches\n",
      "= 32768 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 5119 steps of training\n",
      "Train set accuracy: 0.916046\n",
      "Validation set accuracy: 0.911909\n",
      "Test set accuracy: 0.911909\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 258 batches\n",
      "= 33024 samples\n",
      "= 16 % of the full train dataset size\n",
      "= 5159 steps of training\n",
      "Train set accuracy: 0.912064\n",
      "Validation set accuracy: 0.894753\n",
      "Test set accuracy: 0.894753\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 260 batches\n",
      "= 33280 samples\n",
      "= 17 % of the full train dataset size\n",
      "= 5199 steps of training\n",
      "Train set accuracy: 0.924129\n",
      "Validation set accuracy: 0.918832\n",
      "Test set accuracy: 0.918832\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 262 batches\n",
      "= 33536 samples\n",
      "= 17 % of the full train dataset size\n",
      "= 5239 steps of training\n",
      "Train set accuracy: 0.902016\n",
      "Validation set accuracy: 0.889134\n",
      "Test set accuracy: 0.889134\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 264 batches\n",
      "= 33792 samples\n",
      "= 17 % of the full train dataset size\n",
      "= 5279 steps of training\n",
      "Train set accuracy: 0.920780\n",
      "Validation set accuracy: 0.917528\n",
      "Test set accuracy: 0.917528\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 266 batches\n",
      "= 34048 samples\n",
      "= 17 % of the full train dataset size\n",
      "= 5319 steps of training\n",
      "Train set accuracy: 0.919878\n",
      "Validation set accuracy: 0.915923\n",
      "Test set accuracy: 0.915923\n",
      "Re-initialized \n",
      "\n",
      "\n",
      "==> with a training subset of 268 batches\n",
      "= 34304 samples\n",
      "= 17 % of the full train dataset size\n",
      "= 5359 steps of training\n",
      "Train set accuracy: 0.921642\n",
      "Validation set accuracy: 0.915521\n",
      "Test set accuracy: 0.915521\n",
      "Re-initialized \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-783f7e574c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Feed it to the optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_data\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Log the performance with a downsampled training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # log the quality of the model\n",
    "    train_accuracy_log = tf.scalar_summary(\"train accuracy\", model_accuracy)\n",
    "    valid_accuracy_log = tf.scalar_summary(\"validation accuracy\", model_accuracy)\n",
    "    test_accuracy_log = tf.scalar_summary(\"test accuracy\", model_accuracy)\n",
    "    loss_log = tf.scalar_summary(\"loss\", loss)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/udacity_logs\", session.graph_def)\n",
    "        \n",
    "    for i in range(2, train_dataset.shape[0] // batch_size, 2):\n",
    "        \n",
    "        # Reset all the variables\n",
    "        tf.initialize_all_variables().run()\n",
    "        session.run(global_step.assign(0))\n",
    "        session.run(subset_size.assign(i * batch_size))\n",
    "        print(\"Re-initialized \\n\")\n",
    "        \n",
    "        # Extract a subset from the training dataset\n",
    "        subset_data, subset_labels = random_subset(train_dataset, train_labels, i * batch_size)\n",
    "\n",
    "        # Train with a subset of n_i batch\n",
    "        for step in range(num_epoch * i):\n",
    "            # Generate a minibatch.\n",
    "            offset = (step * batch_size) % (subset_data.shape[0] - batch_size)\n",
    "            batch_data = subset_data[offset:(offset + batch_size), :]\n",
    "            batch_labels = subset_labels[offset:(offset + batch_size), :]\n",
    "            # Feed it to the optimizer\n",
    "            feed_dict = {input_data : batch_data, output_data : batch_labels}\n",
    "            session.run(optimizer, feed_dict=feed_dict)\n",
    "            \n",
    "        # Log the performance with a downsampled training set\n",
    "        print('\\n==> with a training subset of %d batches' % i)\n",
    "        print('= %d samples' % subset_data.shape[0])\n",
    "        print('= %d %% of the full train dataset size' % (100 * subset_data.shape[0] // train_dataset.shape[0]))\n",
    "        print('= %d steps of training' % step)\n",
    "        \n",
    "        #evaluate on the train set\n",
    "        feed_dict = {input_data : subset_data, output_data : subset_labels}\n",
    "        acc, acc_str = session.run([model_accuracy, train_accuracy_log], feed_dict=feed_dict)\n",
    "        writer.add_summary(acc_str, subset_data.shape[0])\n",
    "        print(\"Train set accuracy: %f\" % acc)\n",
    "\n",
    "        #evaluate on the validation set\n",
    "        feed_dict = {input_data : valid_dataset, output_data : valid_labels}\n",
    "        acc, acc_str = session.run([model_accuracy, valid_accuracy_log], feed_dict=feed_dict)\n",
    "        writer.add_summary(acc_str, subset_data.shape[0])\n",
    "        print(\"Validation set accuracy: %f\" % acc)\n",
    "\n",
    "        #evaluate on the test set\n",
    "        feed_dict = {input_data : test_dataset, output_data : test_labels}\n",
    "        acc, acc_str = session.run([model_accuracy, test_accuracy_log], feed_dict=feed_dict)\n",
    "        writer.add_summary(acc_str, subset_data.shape[0])\n",
    "        print(\"Test set accuracy: %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "input_size = image_size * image_size\n",
    "hidden_size = 90\n",
    "output_size = num_labels\n",
    "\n",
    "num_steps = 6000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Generic placeholders : it can either run with ttrain / test / valid data\n",
    "    input_data = tf.placeholder(tf.float32, shape=(None, input_size), name='input')\n",
    "    output_data = tf.placeholder(tf.float32, shape=(None, output_size), name='output')\n",
    "\n",
    "    # First layer : dense\n",
    "    with tf.variable_scope('first_layer') as scope:\n",
    "        layer_1_weights = tf.Variable(tf.truncated_normal([input_size, hidden_size]), name='layer_1_weights')\n",
    "        layer_1_biases = tf.Variable(tf.zeros([hidden_size]), name='layer_1_biases')\n",
    "        layer_1_logits = tf.matmul(input_data, layer_1_weights) + layer_1_biases\n",
    "        layer_1_relu = tf.nn.relu(layer_1_logits, name='layer_1_relu')\n",
    "        \n",
    "    # Second layer : dense\n",
    "    with tf.variable_scope('second_layer') as scope:\n",
    "        layer_2_weights = tf.Variable(tf.truncated_normal([hidden_size, output_size]), name='layer_2_weights')\n",
    "        layer_2_biases = tf.Variable(tf.zeros([output_size]), name='layer_2_biases')\n",
    "        layer_2_logits = tf.matmul(layer_1_relu, layer_2_weights) + layer_2_biases\n",
    "    \n",
    "    # Dropout\n",
    "    with tf.variable_scope('dropout') as scope:\n",
    "        keep_prob = tf.placeholder(\"float\")\n",
    "        layer_2_drop = tf.nn.dropout(layer_2_logits, keep_prob)\n",
    "    \n",
    "    # Output = prediction\n",
    "    with tf.variable_scope('prediction') as scope:\n",
    "        model_output = tf.nn.softmax(layer_2_drop, name='model_prediction')\n",
    "        model_accuracy = accuracy(model_output, output_data, name='model_accuracy')\n",
    "    \n",
    "    # Loss\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(layer_2_drop, output_data), name='loss')\n",
    "    \n",
    "    # Optimizer with dynamic learning rate\n",
    "    with tf.variable_scope('optimizer') as scope:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, num_steps, 0.96)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "==> step 100\n",
      "Train set accuracy: 0.671033\n",
      "Validation set accuracy: 0.747266\n",
      "Test set accuracy: 0.747266\n",
      "==> step 200\n",
      "Train set accuracy: 0.665614\n",
      "Validation set accuracy: 0.729106\n",
      "Test set accuracy: 0.729106\n",
      "==> step 300\n",
      "Train set accuracy: 0.686409\n",
      "Validation set accuracy: 0.747366\n",
      "Test set accuracy: 0.747366\n",
      "==> step 400\n",
      "Train set accuracy: 0.693755\n",
      "Validation set accuracy: 0.752283\n",
      "Test set accuracy: 0.752283\n",
      "==> step 500\n",
      "Train set accuracy: 0.706469\n",
      "Validation set accuracy: 0.767633\n",
      "Test set accuracy: 0.767633\n",
      "==> step 600\n",
      "Train set accuracy: 0.724352\n",
      "Validation set accuracy: 0.783586\n",
      "Test set accuracy: 0.783586\n",
      "==> step 700\n",
      "Train set accuracy: 0.725138\n",
      "Validation set accuracy: 0.803853\n",
      "Test set accuracy: 0.803853\n",
      "==> step 800\n",
      "Train set accuracy: 0.726638\n",
      "Validation set accuracy: 0.806160\n",
      "Test set accuracy: 0.806160\n",
      "==> step 900\n",
      "Train set accuracy: 0.734261\n",
      "Validation set accuracy: 0.799940\n",
      "Test set accuracy: 0.799940\n",
      "==> step 1000\n",
      "Train set accuracy: 0.738571\n",
      "Validation set accuracy: 0.818902\n",
      "Test set accuracy: 0.818902\n",
      "==> step 1100\n",
      "Train set accuracy: 0.749698\n",
      "Validation set accuracy: 0.812882\n",
      "Test set accuracy: 0.812882\n",
      "==> step 1200\n",
      "Train set accuracy: 0.750263\n",
      "Validation set accuracy: 0.812080\n",
      "Test set accuracy: 0.812080\n",
      "==> step 1300\n",
      "Train set accuracy: 0.739326\n",
      "Validation set accuracy: 0.817698\n",
      "Test set accuracy: 0.817698\n",
      "==> step 1400\n",
      "Train set accuracy: 0.750130\n",
      "Validation set accuracy: 0.827631\n",
      "Test set accuracy: 0.827631\n",
      "==> step 1500\n",
      "Train set accuracy: 0.757270\n",
      "Validation set accuracy: 0.835256\n",
      "Test set accuracy: 0.835256\n",
      "==> step 1600\n",
      "Train set accuracy: 0.761621\n",
      "Validation set accuracy: 0.837664\n",
      "Test set accuracy: 0.837664\n",
      "==> step 1700\n",
      "Train set accuracy: 0.762721\n",
      "Validation set accuracy: 0.842882\n",
      "Test set accuracy: 0.842882\n",
      "==> step 1800\n",
      "Train set accuracy: 0.755770\n",
      "Validation set accuracy: 0.817397\n",
      "Test set accuracy: 0.817397\n",
      "==> step 1900\n",
      "Train set accuracy: 0.760296\n",
      "Validation set accuracy: 0.834554\n",
      "Test set accuracy: 0.834554\n",
      "==> step 2000\n",
      "Train set accuracy: 0.745183\n",
      "Validation set accuracy: 0.806261\n",
      "Test set accuracy: 0.806261\n",
      "==> step 2100\n",
      "Train set accuracy: 0.748799\n",
      "Validation set accuracy: 0.810776\n",
      "Test set accuracy: 0.810776\n",
      "==> step 2200\n",
      "Train set accuracy: 0.761832\n",
      "Validation set accuracy: 0.820708\n",
      "Test set accuracy: 0.820708\n",
      "==> step 2300\n",
      "Train set accuracy: 0.768377\n",
      "Validation set accuracy: 0.828635\n",
      "Test set accuracy: 0.828635\n",
      "==> step 2400\n",
      "Train set accuracy: 0.772091\n",
      "Validation set accuracy: 0.849704\n",
      "Test set accuracy: 0.849704\n",
      "==> step 2500\n",
      "Train set accuracy: 0.765094\n",
      "Validation set accuracy: 0.826427\n",
      "Test set accuracy: 0.826427\n",
      "==> step 2600\n",
      "Train set accuracy: 0.778399\n",
      "Validation set accuracy: 0.855222\n",
      "Test set accuracy: 0.855222\n",
      "==> step 2700\n",
      "Train set accuracy: 0.764637\n",
      "Validation set accuracy: 0.842279\n",
      "Test set accuracy: 0.842279\n",
      "==> step 2800\n",
      "Train set accuracy: 0.774351\n",
      "Validation set accuracy: 0.839270\n",
      "Test set accuracy: 0.839270\n",
      "==> step 2900\n",
      "Train set accuracy: 0.771875\n",
      "Validation set accuracy: 0.846393\n",
      "Test set accuracy: 0.846393\n",
      "==> step 3000\n",
      "Train set accuracy: 0.776463\n",
      "Validation set accuracy: 0.850105\n",
      "Test set accuracy: 0.850105\n",
      "==> step 3100\n",
      "Train set accuracy: 0.780043\n",
      "Validation set accuracy: 0.852112\n",
      "Test set accuracy: 0.852112\n",
      "==> step 3200\n",
      "Train set accuracy: 0.776966\n",
      "Validation set accuracy: 0.848701\n",
      "Test set accuracy: 0.848701\n",
      "==> step 3300\n",
      "Train set accuracy: 0.778220\n",
      "Validation set accuracy: 0.852915\n",
      "Test set accuracy: 0.852915\n",
      "==> step 3400\n",
      "Train set accuracy: 0.774028\n",
      "Validation set accuracy: 0.832949\n",
      "Test set accuracy: 0.832949\n",
      "==> step 3500\n",
      "Train set accuracy: 0.784379\n",
      "Validation set accuracy: 0.859336\n",
      "Test set accuracy: 0.859336\n",
      "==> step 3600\n",
      "Train set accuracy: 0.770365\n",
      "Validation set accuracy: 0.832146\n",
      "Test set accuracy: 0.832146\n",
      "==> step 3700\n",
      "Train set accuracy: 0.783804\n",
      "Validation set accuracy: 0.856326\n",
      "Test set accuracy: 0.856326\n",
      "==> step 3800\n",
      "Train set accuracy: 0.786049\n",
      "Validation set accuracy: 0.862446\n",
      "Test set accuracy: 0.862446\n",
      "==> step 3900\n",
      "Train set accuracy: 0.785879\n",
      "Validation set accuracy: 0.861844\n",
      "Test set accuracy: 0.861844\n",
      "==> step 4000\n",
      "Train set accuracy: 0.780613\n",
      "Validation set accuracy: 0.858433\n",
      "Test set accuracy: 0.858433\n",
      "==> step 4100\n",
      "Train set accuracy: 0.785052\n",
      "Validation set accuracy: 0.849604\n",
      "Test set accuracy: 0.849604\n",
      "==> step 4200\n",
      "Train set accuracy: 0.781030\n",
      "Validation set accuracy: 0.838467\n",
      "Test set accuracy: 0.838467\n",
      "==> step 4300\n",
      "Train set accuracy: 0.789249\n",
      "Validation set accuracy: 0.862747\n",
      "Test set accuracy: 0.862747\n",
      "==> step 4400\n",
      "Train set accuracy: 0.787713\n",
      "Validation set accuracy: 0.860841\n",
      "Test set accuracy: 0.860841\n",
      "==> step 4500\n",
      "Train set accuracy: 0.786829\n",
      "Validation set accuracy: 0.860038\n",
      "Test set accuracy: 0.860038\n",
      "==> step 4600\n",
      "Train set accuracy: 0.788499\n",
      "Validation set accuracy: 0.861443\n",
      "Test set accuracy: 0.861443\n",
      "==> step 4700\n",
      "Train set accuracy: 0.789085\n",
      "Validation set accuracy: 0.862747\n",
      "Test set accuracy: 0.862747\n",
      "==> step 4800\n",
      "Train set accuracy: 0.792090\n",
      "Validation set accuracy: 0.849704\n",
      "Test set accuracy: 0.849704\n",
      "==> step 4900\n",
      "Train set accuracy: 0.791448\n",
      "Validation set accuracy: 0.864954\n",
      "Test set accuracy: 0.864954\n",
      "==> step 5000\n",
      "Train set accuracy: 0.791335\n",
      "Validation set accuracy: 0.845891\n",
      "Test set accuracy: 0.845891\n",
      "==> step 5100\n",
      "Train set accuracy: 0.793944\n",
      "Validation set accuracy: 0.849503\n",
      "Test set accuracy: 0.849503\n",
      "==> step 5200\n",
      "Train set accuracy: 0.794715\n",
      "Validation set accuracy: 0.867162\n",
      "Test set accuracy: 0.867162\n",
      "==> step 5300\n",
      "Train set accuracy: 0.793518\n",
      "Validation set accuracy: 0.868566\n",
      "Test set accuracy: 0.868566\n",
      "==> step 5400\n",
      "Train set accuracy: 0.797201\n",
      "Validation set accuracy: 0.873081\n",
      "Test set accuracy: 0.873081\n",
      "==> step 5500\n",
      "Train set accuracy: 0.793169\n",
      "Validation set accuracy: 0.864252\n",
      "Test set accuracy: 0.864252\n",
      "==> step 5600\n",
      "Train set accuracy: 0.795075\n",
      "Validation set accuracy: 0.867362\n",
      "Test set accuracy: 0.867362\n",
      "==> step 5700\n",
      "Train set accuracy: 0.782047\n",
      "Validation set accuracy: 0.854821\n",
      "Test set accuracy: 0.854821\n",
      "==> step 5800\n",
      "Train set accuracy: 0.792121\n",
      "Validation set accuracy: 0.863550\n",
      "Test set accuracy: 0.863550\n",
      "==> step 5900\n",
      "Train set accuracy: 0.797972\n",
      "Validation set accuracy: 0.870473\n",
      "Test set accuracy: 0.870473\n",
      "==> step 6000\n",
      "Train set accuracy: 0.787281\n",
      "Validation set accuracy: 0.860339\n",
      "Test set accuracy: 0.860339\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # log the quality of the model\n",
    "    train_accuracy_log = tf.scalar_summary(\"train accuracy\", model_accuracy)\n",
    "    valid_accuracy_log = tf.scalar_summary(\"validation accuracy\", model_accuracy)\n",
    "    test_accuracy_log = tf.scalar_summary(\"test accuracy\", model_accuracy)\n",
    "    loss_log = tf.scalar_summary(\"loss\", loss)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/udacity_logs\", session.graph_def)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(1,num_steps+1):\n",
    "        if (step % 100 == 0):\n",
    "            print(\"==> step %d\" % step)\n",
    "            \n",
    "            #evaluate on the train set\n",
    "            feed_dict = {input_data : train_dataset, output_data : train_labels, keep_prob: 1.0}\n",
    "            acc, acc_str = session.run([model_accuracy, train_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Train set accuracy: %f\" % acc)\n",
    "            \n",
    "            #evaluate on the validation set\n",
    "            feed_dict = {input_data : valid_dataset, output_data : valid_labels, keep_prob: 1.0}\n",
    "            acc, acc_str = session.run([model_accuracy, valid_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Validation set accuracy: %f\" % acc)\n",
    "            \n",
    "            #evaluate on the test set\n",
    "            feed_dict = {input_data : test_dataset, output_data : test_labels, keep_prob: 1.0}\n",
    "            acc, acc_str = session.run([model_accuracy, test_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Test set accuracy: %f\" % acc)\n",
    "        else:\n",
    "            # Generate a minibatch.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {input_data : batch_data, output_data : batch_labels, keep_prob: 0.5}\n",
    "            session.run(optimizer, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "input_size = image_size * image_size\n",
    "hidden_1_size = 180\n",
    "hidden_2_size = 40\n",
    "output_size = num_labels\n",
    "\n",
    "num_steps = 6000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Generic placeholders : it can either run with ttrain / test / valid data\n",
    "    input_data = tf.placeholder(tf.float32, shape=(None, input_size), name='input')\n",
    "    output_data = tf.placeholder(tf.float32, shape=(None, output_size), name='output')\n",
    "\n",
    "    # First layer : dense\n",
    "    with tf.variable_scope('first_layer') as scope:\n",
    "        layer_1_weights = tf.Variable(tf.truncated_normal([input_size, hidden_1_size]), name='layer_1_weights')\n",
    "        layer_1_biases = tf.Variable(tf.zeros([hidden_1_size]), name='layer_1_biases')\n",
    "        layer_1_logits = tf.matmul(input_data, layer_1_weights) + layer_1_biases\n",
    "        layer_1_relu = tf.nn.relu(layer_1_logits, name='layer_1_relu')\n",
    "        \n",
    "    # Second layer : dense\n",
    "    with tf.variable_scope('second_layer') as scope:\n",
    "        layer_2_weights = tf.Variable(tf.truncated_normal([hidden_1_size, hidden_2_size]), name='layer_2_weights')\n",
    "        layer_2_biases = tf.Variable(tf.zeros([hidden_2_size]), name='layer_2_biases')\n",
    "        layer_2_logits = tf.matmul(layer_1_relu, layer_2_weights) + layer_2_biases\n",
    "        layer_2_relu = tf.nn.relu(layer_2_logits, name='layer_2_relu')\n",
    "    \n",
    "    # Dropout\n",
    "    with tf.variable_scope('dropout') as scope:\n",
    "        keep_prob = tf.placeholder(\"float\")\n",
    "        layer_2_drop = tf.nn.dropout(layer_2_relu, keep_prob)\n",
    "        \n",
    "    # Third layer : dense\n",
    "    with tf.variable_scope('third_layer') as scope:\n",
    "        layer_3_weights = tf.Variable(tf.truncated_normal([hidden_2_size, output_size]), name='layer_3_weights')\n",
    "        layer_3_biases = tf.Variable(tf.zeros([output_size]), name='layer_3_biases')\n",
    "        layer_3_logits = tf.matmul(layer_2_drop, layer_3_weights) + layer_3_biases\n",
    "    \n",
    "    # Output = prediction\n",
    "    with tf.variable_scope('prediction') as scope:\n",
    "        model_output = tf.nn.softmax(layer_3_logits, name='model_prediction')\n",
    "        model_accuracy = accuracy(model_output, output_data, name='model_accuracy')\n",
    "    \n",
    "    # Loss\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(layer_3_logits, output_data), name='loss')\n",
    "    \n",
    "    # Optimizer with dynamic learning rate\n",
    "    with tf.variable_scope('optimizer') as scope:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, num_steps, 0.96)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "==> step 100\n",
      "Train set accuracy: 0.101351\n",
      "Validation set accuracy: 0.100231\n",
      "Test set accuracy: 0.100231\n",
      "==> step 200\n",
      "Train set accuracy: 0.101351\n",
      "Validation set accuracy: 0.100231\n",
      "Test set accuracy: 0.100231\n",
      "==> step 300\n",
      "Train set accuracy: 0.101279\n",
      "Validation set accuracy: 0.100130\n",
      "Test set accuracy: 0.100130\n",
      "==> step 400\n",
      "Train set accuracy: 0.101037\n",
      "Validation set accuracy: 0.099428\n",
      "Test set accuracy: 0.099428\n",
      "==> step 500\n",
      "Train set accuracy: 0.101263\n",
      "Validation set accuracy: 0.100130\n",
      "Test set accuracy: 0.100130\n",
      "==> step 600\n",
      "Train set accuracy: 0.101351\n",
      "Validation set accuracy: 0.100231\n",
      "Test set accuracy: 0.100231\n",
      "==> step 700\n",
      "Train set accuracy: 0.101186\n",
      "Validation set accuracy: 0.099930\n",
      "Test set accuracy: 0.099930\n",
      "==> step 800\n",
      "Train set accuracy: 0.101279\n",
      "Validation set accuracy: 0.100130\n",
      "Test set accuracy: 0.100130\n",
      "==> step 900\n",
      "Train set accuracy: 0.101212\n",
      "Validation set accuracy: 0.099829\n",
      "Test set accuracy: 0.099829\n",
      "==> step 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6a5c03a51fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m#evaluate on the train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_data\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_data\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy_log\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train set accuracy: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mcm/Documents/Workspace/hacks/ai/udacity/env/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    \n",
    "    # log the quality of the model\n",
    "    train_accuracy_log = tf.scalar_summary(\"train accuracy\", model_accuracy)\n",
    "    valid_accuracy_log = tf.scalar_summary(\"validation accuracy\", model_accuracy)\n",
    "    test_accuracy_log = tf.scalar_summary(\"test accuracy\", model_accuracy)\n",
    "    loss_log = tf.scalar_summary(\"loss\", loss)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs\n",
    "    writer = tf.train.SummaryWriter(\"/tmp/udacity_logs\", session.graph_def)\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(1,num_steps+1):\n",
    "        if (step % 100 == 0):\n",
    "            print(\"==> step %d\" % step)\n",
    "            \n",
    "            #evaluate on the train set\n",
    "            feed_dict = {input_data : train_dataset, output_data : train_labels, keep_prob: 1.0}\n",
    "            acc, acc_str = session.run([model_accuracy, train_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Train set accuracy: %f\" % acc)\n",
    "            \n",
    "            #evaluate on the validation set\n",
    "            feed_dict = {input_data : valid_dataset, output_data : valid_labels, keep_prob: 1.0}\n",
    "            acc, acc_str = session.run([model_accuracy, valid_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Validation set accuracy: %f\" % acc)\n",
    "            \n",
    "            #evaluate on the test set\n",
    "            feed_dict = {input_data : test_dataset, output_data : test_labels, keep_prob: 1.0}\n",
    "            acc, acc_str = session.run([model_accuracy, test_accuracy_log], feed_dict=feed_dict)\n",
    "            writer.add_summary(acc_str, step)\n",
    "            print(\"Test set accuracy: %f\" % acc)\n",
    "        else:\n",
    "            # Generate a minibatch.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {input_data : batch_data, output_data : batch_labels, keep_prob: 0.5}\n",
    "            session.run(optimizer, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
